from __future__ import annotations
import json
import os
import time
import uuid
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse

from src.worker_brain import run_worker   # MiniAIWorker replaces executor directly
from src.training_loader import seed_from_training_data, is_stale
from src.context_rl import get_context_stats
from src.dynamic_fsm import get_synthesis_stats
from src.report_analyzer import analyze_and_save, load_intelligence

app = FastAPI(title="BrainOS Purple Agent", version="2.0.0")

AGENT_CARD = {
    "name": "BrainOS Purple Agent",
    "description": (
        "Mini AI Worker — a competition-focused distillation of the BrainOS AI Worker. "
        "8-state FSM, deterministic policy enforcement, Haiku memory compression, "
        "financial arithmetic, schema drift resilience, and RL quality loop."
    ),
    "version": "2.0.0",
    "url": os.getenv("PURPLE_AGENT_CARD_URL", "https://purple.agentbench.usebrainos.com"),
    "capabilities": {"streaming": False, "tools": True},
    "skills": [{
        "id": "business-process",
        "name": "Business Process AI Worker",
        "description": (
            "End-to-end business process execution: expense approval, procurement, "
            "offboarding, invoice reconciliation, SLA breach, order management, "
            "compliance audit, dispute resolution, AR collections, month-end close."
        ),
    }],
}


@app.on_event("startup")
async def on_startup():
    """
    Background training seed on startup.
    Seeds RL case log from S3/HTTP benchmark data so PRIME phase has learned patterns
    from the first task (not just after the first benchmark round).
    Non-blocking — agent serves requests immediately; seed runs in background thread.
    """
    import threading
    def _seed():
        try:
            seed_from_training_data(force=False)
        except Exception:
            pass  # Never crash startup — agent runs degraded if seed fails
    threading.Thread(target=_seed, daemon=True).start()


@app.get("/.well-known/agent-card.json")
async def agent_card():
    return JSONResponse(AGENT_CARD)


@app.get("/health")
async def health():
    return {"status": "ok", "agent": "brainos-mini-ai-worker", "version": "2.0.0"}


@app.post("/")
async def a2a_handler(request: Request):
    body = await request.json()

    # JSON-RPC 2.0: correlation id lives at top level of request
    jsonrpc_id = body.get("id")

    if body.get("method") != "tasks/send":
        return JSONResponse({
            "jsonrpc": "2.0", "id": jsonrpc_id,
            "error": {"code": -32601, "message": "Method not found"},
        })

    params = body.get("params", {})
    task_id = params.get("id", str(uuid.uuid4()))
    message = params.get("message", {})
    metadata = params.get("metadata", {})

    task_text = "".join(p.get("text", "") for p in message.get("parts", []))
    policy_doc = metadata.get("policy_doc", "")
    tools_endpoint = metadata.get("tools_endpoint", "")
    session_id = metadata.get("session_id", task_id)

    try:
        answer = await run_worker(
            task_text=task_text,
            policy_doc=policy_doc,
            tools_endpoint=tools_endpoint,
            task_id=task_id,
            session_id=session_id,
        )
    except Exception as exc:
        # Return JSON-RPC error (not 500 HTML) so benchmark evaluator can parse it
        return JSONResponse({
            "jsonrpc": "2.0",
            "id": jsonrpc_id,
            "error": {"code": -32603, "message": f"Internal error: {exc}", "data": None},
        })

    return JSONResponse({
        "jsonrpc": "2.0",
        "id": jsonrpc_id,   # required by JSON-RPC 2.0 spec
        "result": {
            "id": task_id,
            "status": {"state": "completed"},
            "artifacts": [{"parts": [{"text": answer}]}],
        },
    })


@app.get("/rl/status")
async def rl_status():
    """
    RL and knowledge base status endpoint.
    Returns case log stats, knowledge base summary, entity memory stats,
    and knowledge growth rate from the growth log.

    New fields added (2026-03-01):
      - knowledge_base.total_entries
      - knowledge_base.domains_covered
      - knowledge_base.growth_rate (computed from knowledge_growth.log)
      - knowledge_base.last_extraction
      - entity_memory.total_entities
      - entity_memory.recurring_entities (seen_count >= 2)
    """
    base_dir = os.path.join(os.path.dirname(__file__), "..")

    # ── Case log stats ────────────────────────────────────────────────────────
    case_log_path = os.path.join(base_dir, "case_log.json")
    case_stats: dict = {"total": 0, "successes": 0, "failures": 0, "avg_quality": 0.0}
    try:
        if os.path.exists(case_log_path):
            with open(case_log_path) as f:
                cases = json.load(f)
            if cases:
                qualities = [c.get("quality", 0) for c in cases]
                case_stats = {
                    "total": len(cases),
                    "successes": sum(1 for c in cases if c.get("outcome") == "success"),
                    "failures": sum(1 for c in cases if c.get("outcome") == "failure"),
                    "avg_quality": round(sum(qualities) / len(qualities), 3),
                }
    except Exception:
        pass

    # ── Knowledge base stats ──────────────────────────────────────────────────
    kb_path = os.path.join(base_dir, "knowledge_base.json")
    kb_stats: dict = {
        "total_entries": 0,
        "domains_covered": [],
        "growth_rate": "0 entries/hour",
        "last_extraction": None,
        "by_extraction_method": {},
    }
    try:
        if os.path.exists(kb_path):
            with open(kb_path) as f:
                kb_entries = json.load(f)
            domains = list({e.get("domain", "unknown") for e in kb_entries})
            methods: dict[str, int] = {}
            for e in kb_entries:
                m = e.get("extraction_method", "haiku")
                methods[m] = methods.get(m, 0) + 1
            last_ts = max((e.get("created_at", 0) for e in kb_entries), default=0)
            kb_stats = {
                "total_entries": len(kb_entries),
                "domains_covered": sorted(domains),
                "growth_rate": _compute_growth_rate(base_dir),
                "last_extraction": (
                    time.strftime("%Y-%m-%dT%H:%M:%S", time.localtime(last_ts))
                    if last_ts else None
                ),
                "by_extraction_method": methods,
            }
    except Exception:
        pass

    # ── Entity memory stats ───────────────────────────────────────────────────
    entity_path = os.path.join(base_dir, "entity_memory.json")
    entity_stats: dict = {"total_entities": 0, "recurring_entities": 0, "by_type": {}}
    try:
        if os.path.exists(entity_path):
            with open(entity_path) as f:
                entity_records = json.load(f)
            type_counts: dict[str, int] = {}
            for r in entity_records:
                t = r.get("entity_type", "unknown")
                type_counts[t] = type_counts.get(t, 0) + 1
            entity_stats = {
                "total_entities": len(entity_records),
                "recurring_entities": sum(
                    1 for r in entity_records if r.get("seen_count", 1) >= 2
                ),
                "by_type": type_counts,
            }
    except Exception:
        pass

    # Context injection quality stats (Wave 12)
    ctx_stats: dict = {}
    try:
        ctx_stats = get_context_stats()
    except Exception:
        pass

    return {
        # Top-level aliases for smoke test / legacy clients
        "status": "ok",
        "total_cases": case_stats.get("total", 0),
        "avg_quality": case_stats.get("avg_quality", 0.0),
        # Namespaced detail
        "case_log": case_stats,
        "knowledge_base": kb_stats,
        "entity_memory": entity_stats,
        "context_rl": ctx_stats,        # per-process confidence + drift alerts
        "dynamic_fsm": get_synthesis_stats(),  # Wave 13: novel type synthesis cache
    }


@app.get("/training/status")
async def training_status():
    """S3 training seed status — was the case log primed from benchmark data?"""
    base_dir = os.path.join(os.path.dirname(__file__), "..")
    seeded_marker = os.path.join(base_dir, ".training_seeded")
    stale = True
    try:
        stale = is_stale()
    except Exception:
        pass
    intel: dict = {}
    try:
        intel = load_intelligence()
    except Exception:
        pass
    return {
        "status": "ok",
        "seeded": os.path.exists(seeded_marker),
        "stale": stale,
        "benchmark_intelligence": intel,
    }


@app.post("/training/sync")
async def training_sync():
    """Force refresh from S3 / HTTP benchmark endpoint."""
    results: dict = {}
    try:
        seed_result = seed_from_training_data(force=True)
        results["seed"] = seed_result
    except Exception as e:
        results["seed_error"] = str(e)
    try:
        analyze_result = analyze_and_save(force=True)
        results["analyze"] = analyze_result
    except Exception as e:
        results["analyze_error"] = str(e)
    results["status"] = "ok" if not any("error" in k for k in results) else "partial"
    return results


def _compute_growth_rate(base_dir: str) -> str:
    """
    Compute knowledge growth rate from the last 10 lines of knowledge_growth.log.
    Returns a human-readable string like "3 entries/hour".
    """
    growth_log = os.path.join(base_dir, "knowledge_growth.log")
    try:
        if not os.path.exists(growth_log):
            return "0 entries/hour"

        with open(growth_log) as f:
            lines = f.readlines()

        # Take the last 10 entries
        recent = [ln.strip() for ln in lines[-10:] if ln.strip()]
        if not recent:
            return "0 entries/hour"

        # Parse timestamps from log lines: "2026-03-01T12:34:56 domain=... new=N total=N"
        import re as _re
        entries_total = 0
        first_ts = None
        last_ts = None

        for ln in recent:
            ts_m = _re.match(r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2})', ln)
            new_m = _re.search(r'new=(\d+)', ln)
            if ts_m and new_m:
                ts_str = ts_m.group(1)
                ts = time.mktime(time.strptime(ts_str, "%Y-%m-%dT%H:%M:%S"))
                new = int(new_m.group(1))
                entries_total += new
                if first_ts is None:
                    first_ts = ts
                last_ts = ts

        if first_ts is None or last_ts is None or last_ts == first_ts:
            return f"{entries_total} entries (window too short)"

        elapsed_hours = (last_ts - first_ts) / 3600
        if elapsed_hours < 0.01:
            return f"{entries_total} entries (window too short)"

        rate = entries_total / elapsed_hours
        return f"{rate:.1f} entries/hour"

    except Exception:
        return "unknown"
